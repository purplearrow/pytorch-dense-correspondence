{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import dense_correspondence_manipulation.utils.utils as utils\n",
    "utils.add_dense_correspondence_to_python_path()\n",
    "\n",
    "import dense_correspondence\n",
    "from dense_correspondence.evaluation.evaluation import *\n",
    "import dense_correspondence.correspondence_tools.correspondence_plotter as correspondence_plotter\n",
    "from dense_correspondence.dataset.dense_correspondence_dataset_masked import ImageType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', \n",
    "                               'dense_correspondence', 'evaluation', 'evaluation.yaml')\n",
    "config = utils.getDictFromYamlFilename(config_filename)\n",
    "default_config = utils.get_defaults_config()\n",
    "\n",
    "# utils.set_cuda_visible_devices([0])\n",
    "dce = DenseCorrespondenceEvaluation(config)\n",
    "DCE = DenseCorrespondenceEvaluation\n",
    "\n",
    "network_name = \"robot_box_3\"\n",
    "dcn = dce.load_network_from_config(network_name)\n",
    "dataset = dcn.load_training_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code of original author\n",
    "#DenseCorrespondenceEvaluation.evaluate_network_qualitative(dcn, dataset=dataset, randomize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dense_correspondence.correspondence_tools.correspondence_finder import random_sample_from_masked_image\n",
    "def draw_compare_original(dcn, dataset, scene_name, img_a_idx, img_b_idx, num_matches=10):\n",
    "    \"\"\"copy from single_image_pair_qualitative_analysis\"\"\"\n",
    "    rgb_a, _, mask_a, _ = dataset.get_rgbd_mask_pose(scene_name, img_a_idx)\n",
    "    rgb_b, _, mask_b, _ = dataset.get_rgbd_mask_pose(scene_name, img_b_idx)\n",
    "\n",
    "    mask_a = np.asarray(mask_a)\n",
    "    mask_b = np.asarray(mask_b)\n",
    "\n",
    "    # compute dense descriptors\n",
    "    rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n",
    "    rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n",
    "\n",
    "    # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n",
    "    res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n",
    "    #load here\n",
    "    res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n",
    "    #res_b = DenseCorrespondenceEvaluation.load_pre_computed_feature_map(scene_name, img_b_idx)\n",
    "    print (scene_name)\n",
    "\n",
    "\n",
    "    # sample points on img_a. Compute best matches on img_b\n",
    "    # note that this is in (x,y) format\n",
    "    # TODO: if this mask is empty, this function will not be happy\n",
    "    # de-prioritizing since this is only for qualitative evaluation plots\n",
    "    sampled_idx_list = random_sample_from_masked_image(mask_a, num_matches)\n",
    "\n",
    "    # list of cv2.KeyPoint\n",
    "    kp1 = []\n",
    "    kp2 = []\n",
    "    matches = []  # list of cv2.DMatch\n",
    "\n",
    "    # placeholder constants for opencv\n",
    "    diam = 0.01\n",
    "    dist = 0.01\n",
    "    \n",
    "    for i in xrange(0, num_matches):\n",
    "        # convert to (u,v) format\n",
    "        pixel_a = [sampled_idx_list[1][i], sampled_idx_list[0][i]]\n",
    "        best_match_uv, best_match_diff, norm_diffs =\\\n",
    "            DenseCorrespondenceNetwork.find_best_match(pixel_a, res_a, res_b)\n",
    "\n",
    "        # be careful, OpenCV format is  (u,v) = (right, down)\n",
    "        kp1.append(cv2.KeyPoint(pixel_a[0], pixel_a[1], diam))\n",
    "        kp2.append(cv2.KeyPoint(best_match_uv[0], best_match_uv[1], diam))\n",
    "        matches.append(cv2.DMatch(i, i, dist))\n",
    "        \n",
    "    gray_a_numpy = cv2.cvtColor(np.asarray(rgb_a), cv2.COLOR_BGR2GRAY)\n",
    "    gray_b_numpy = cv2.cvtColor(np.asarray(rgb_b), cv2.COLOR_BGR2GRAY)\n",
    "    img3 = cv2.drawMatches(gray_a_numpy, kp1, gray_b_numpy, kp2, matches, flags=2, outImg=gray_b_numpy)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(15)\n",
    "    axes.imshow(img3)\n",
    "\n",
    "    return sampled_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_compare_flow(dcn, dataset, scene_name, img_a_idx, img_b_idx, flow_threshold, sampled_idx_list=None, num_matches=10):\n",
    "    \"\"\"copy from single_image_pair_qualitative_analysis\"\"\"\n",
    "    rgb_a, _, mask_a, _ = dataset.get_rgbd_mask_pose(scene_name, img_a_idx)\n",
    "    rgb_b, _, mask_b, _ = dataset.get_rgbd_mask_pose(scene_name, img_b_idx)\n",
    "\n",
    "    mask_a = np.asarray(mask_a)\n",
    "    mask_b = np.asarray(mask_b)\n",
    "\n",
    "    # compute dense descriptors\n",
    "    rgb_a_tensor = dataset.rgb_image_to_tensor(rgb_a)\n",
    "    rgb_b_tensor = dataset.rgb_image_to_tensor(rgb_b)\n",
    "\n",
    "    # these are Variables holding torch.FloatTensors, first grab the data, then convert to numpy\n",
    "    res_a = dcn.forward_single_image_tensor(rgb_a_tensor).data.cpu().numpy()\n",
    "    #load here\n",
    "    #res_b = dcn.forward_single_image_tensor(rgb_b_tensor).data.cpu().numpy()\n",
    "    res_b = DenseCorrespondenceEvaluation.load_pre_computed_feature_map(scene_name, img_b_idx)\n",
    "    \n",
    "    load_feature_map_folder = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'data_volume', 'pdc',\n",
    "                                'flow_errormap', 'example', 'itri_box')\n",
    "    load_res_b_f, load_res_b_n = DenseCorrespondenceEvaluation.load_nf_feature(load_feature_map_folder, scene_name, img_b_idx)\n",
    "\n",
    "    # sample points on img_a. Compute best matches on img_b\n",
    "    # note that this is in (x,y) format\n",
    "    # TODO: if this mask is empty, this function will not be happy\n",
    "    # de-prioritizing since this is only for qualitative evaluation plots\n",
    "    if not sampled_idx_list:\n",
    "        sampled_idx_list = random_sample_from_masked_image(mask_a, num_matches)\n",
    "    else:\n",
    "        print (\"use sampled_idx_list from input\")\n",
    "    print (\"number of sampled_idx_list: %d\" %len(sampled_idx_list[0]))\n",
    "\n",
    "    # list of cv2.KeyPoint\n",
    "    kp1 = []\n",
    "    kp2 = []\n",
    "    matches = []  # list of cv2.DMatch\n",
    "\n",
    "    # placeholder constants for opencv\n",
    "    diam = 0.01\n",
    "    dist = 0.01\n",
    "\n",
    "    number_wanted = 0\n",
    "    for i in xrange(0, num_matches):\n",
    "        # convert to (u,v) format\n",
    "        pixel_a = [sampled_idx_list[1][i], sampled_idx_list[0][i]]\n",
    "        best_match_uv, best_match_diff, norm_diffs =\\\n",
    "            DenseCorrespondenceNetwork.find_best_match(pixel_a, res_a, res_b)\n",
    "\n",
    "        # compute best match\n",
    "        ## between a & b\n",
    "        uv_b_pred, best_match_diff, norm_diffs =\\\n",
    "            DenseCorrespondenceNetwork.find_best_match(pixel_a, res_a,\n",
    "                                                       res_b, debug=False)\n",
    "        ## between a & b-1\n",
    "        uv_bf_pred, best_match_diff_a_bf, norm_diffs_a_bf =\\\n",
    "            DenseCorrespondenceNetwork.find_best_match(pixel_a, res_a,\n",
    "                                                       load_res_b_f, debug=False)\n",
    "        \n",
    "        ## between a & b+1\n",
    "        uv_bn_pred, best_match_diff_a_bn, norm_diffs_a_bn =\\\n",
    "            DenseCorrespondenceNetwork.find_best_match(pixel_a, res_a,\n",
    "                                                       load_res_b_n, debug=False)\n",
    "\n",
    "        # compute flow(uv_bf_pred) & flow(uv_b_pred)\n",
    "        load_flo_folder = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'data_volume', 'pdc',\n",
    "                                    'flow_errormap', 'example', 'inference', 'itri_box')\n",
    "        flow_uv_bf_pred = DenseCorrespondenceEvaluation.select_pixel_flow(load_flo_folder, scene_name, 'rgbf-rgb', img_b_idx, uv_bf_pred)\n",
    "        flow_uv_b_pred = DenseCorrespondenceEvaluation.select_pixel_flow(load_flo_folder, scene_name, 'rgb-rgbn', img_b_idx, uv_b_pred)\n",
    "        \n",
    "        # compute predicted point distance transformation. (uv_b_pred - uv_bf_pred) & (uv_bn_pred - uv_b_pred)\n",
    "        dist_b_bf = np.zeros(2, np.float64)\n",
    "        dist_bn_b = np.zeros(2, np.float64)\n",
    "        dist_b_bf[0] = uv_b_pred[0] - uv_bf_pred[0]\n",
    "        dist_b_bf[1] = uv_b_pred[1] - uv_bf_pred[1]\n",
    "        dist_bn_b[0] = uv_bn_pred[0] - uv_b_pred[0]\n",
    "        dist_bn_b[1] = uv_bn_pred[1] - uv_b_pred[1]\n",
    "        \n",
    "        error_flow = 0.5*np.linalg.norm(flow_uv_bf_pred - dist_b_bf)+0.5*np.linalg.norm(flow_uv_b_pred - dist_bn_b)\n",
    "        \n",
    "        if error_flow < flow_threshold:\n",
    "            kp1.append(cv2.KeyPoint(pixel_a[0], pixel_a[1], diam))\n",
    "            kp2.append(cv2.KeyPoint(best_match_uv[0], best_match_uv[1], diam))\n",
    "            matches.append(cv2.DMatch(number_wanted, number_wanted, dist))\n",
    "            number_wanted += 1\n",
    "\n",
    "    print (\"number point: %d %d\" % (len(kp1), len(kp2)))\n",
    "    gray_a_numpy = cv2.cvtColor(np.asarray(rgb_a), cv2.COLOR_BGR2GRAY)\n",
    "    gray_b_numpy = cv2.cvtColor(np.asarray(rgb_b), cv2.COLOR_BGR2GRAY)\n",
    "    img3 = cv2.drawMatches(gray_a_numpy, kp1, gray_b_numpy, kp2, matches, flags=2, outImg=gray_b_numpy)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(15)\n",
    "    axes.imshow(img3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_name = \"2018-11-07-16-18-28\"\n",
    "img_a_idx = 3\n",
    "img_b_idx = 21\n",
    "prevous_sampled_idx_list = draw_compare_original(dcn, dataset, scene_name, img_a_idx, img_b_idx, num_matches=50)\n",
    "draw_compare_flow(dcn, dataset, scene_name, img_a_idx, img_b_idx, \n",
    "                  flow_threshold = 8.43, sampled_idx_list=prevous_sampled_idx_list, num_matches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_name = \"2018-11-07-17-29-42\"\n",
    "img_a_idx = 2\n",
    "img_b_idx = 18\n",
    "prevous_sampled_idx_list = draw_compare_original(dcn, dataset, scene_name, img_a_idx, img_b_idx, num_matches=100)\n",
    "draw_compare_flow(dcn, dataset, scene_name, img_a_idx, img_b_idx, \n",
    "                  flow_threshold = 8.43, sampled_idx_list=prevous_sampled_idx_list, num_matches=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
